{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACEBOOK SCRAPER FUNCTION AND RAW DATA FORMAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer: Due to Facebook's mechanism, the following function may be outdated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacebookScraper:\n",
    "    def __init__(self, username, password):\n",
    "        self.driver = self.initialize_driver()\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "\n",
    "    def initialize_driver(self):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--disable-notifications\")\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        return webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    def login(self):\n",
    "        self.driver.get('https://facebook.com')\n",
    "        wait = WebDriverWait(self.driver, 10)\n",
    "        email_field = wait.until(EC.presence_of_element_located((By.ID, \"email\")))\n",
    "        email_field.send_keys(self.username)\n",
    "        self.driver.find_element(By.ID, \"pass\").send_keys(self.password, Keys.RETURN)\n",
    "        time.sleep(5)  # Wait for the login process to complete\n",
    "\n",
    "    def parse_post_date(self, post):\n",
    "        try:\n",
    "            timestamp_element = post.find_element(By.CSS_SELECTOR, 'abbr')\n",
    "            post_date = timestamp_element.text.strip()\n",
    "        except NoSuchElementException:\n",
    "            post_date = None\n",
    "        return post_date\n",
    "\n",
    "    def extract_reaction_type(self, post):\n",
    "        a_tags = post.find_elements(By.TAG_NAME, 'a')\n",
    "        expression_types = None\n",
    "        for a_tag in a_tags:\n",
    "            aria_label = a_tag.get_attribute('aria-label')\n",
    "            if aria_label and 'cảm xúc' in aria_label:\n",
    "                expression_types = aria_label.split(\", bao gồm \")[1] if \", bao gồm \" in aria_label else None\n",
    "        return expression_types\n",
    "\n",
    "    def extract_reaction_count(self, post):\n",
    "        reaction_counts = []\n",
    "        for img_tag in post.find_elements(By.TAG_NAME, 'img'):\n",
    "            parent = img_tag.find_element(By.XPATH, \"..\")\n",
    "            while parent:\n",
    "                if parent.tag_name == 'a':\n",
    "                    text = parent.text.strip().replace('.', '')\n",
    "                    if text and text[0].isdigit():\n",
    "                        try:\n",
    "                            count = int(text.split()[0])\n",
    "                            reaction_counts.append(count)\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                    break\n",
    "                parent = parent.find_element(By.XPATH, \"..\")\n",
    "        return reaction_counts[0] if reaction_counts else 0\n",
    "\n",
    "    def extract_comment_count(self, post):\n",
    "        a_tags = post.find_elements(By.TAG_NAME, 'a')\n",
    "        for a_tag in a_tags:\n",
    "            text = a_tag.text.strip()\n",
    "            if 'bình luận' in text:\n",
    "                try:\n",
    "                    return int(text.split()[0].replace('.', ''))\n",
    "                except ValueError:\n",
    "                    return 0\n",
    "        return 0\n",
    "\n",
    "    def extract_post_link(self, post):\n",
    "        try:\n",
    "            a_tags = post.find_elements(By.TAG_NAME, 'a')\n",
    "            for a_tag in a_tags:\n",
    "                href = a_tag.get_attribute('href')\n",
    "                if href and '/story.php?' in href:\n",
    "                    return href\n",
    "        except NoSuchElementException:\n",
    "            return None\n",
    "\n",
    "    def click_see_more_link(self):\n",
    "        try:\n",
    "            see_more_link = WebDriverWait(self.driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//a[span[text()='Xem tin khác']]\"))\n",
    "            )\n",
    "            see_more_link.click()\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"No 'Xem tin khác' link found: {e}\")\n",
    "\n",
    "    def scrape_facebook_page(self, page_limit=10):\n",
    "        data = []\n",
    "        CSS_SELECTORS = ['article._55wo._56bf._5rgl', 'article.bn.bo.bp']\n",
    "\n",
    "        for page_num in range(page_limit):\n",
    "            html_content = self.driver.page_source\n",
    "\n",
    "            for css_selector in CSS_SELECTORS:\n",
    "                articles = self.driver.find_elements(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "                for article in articles:\n",
    "                    post_date = self.parse_post_date(article)\n",
    "                    reaction_count = self.extract_reaction_count(article)\n",
    "                    top_react = self.extract_reaction_type(article)\n",
    "                    comment_count = self.extract_comment_count(article)\n",
    "                    post_link = self.extract_post_link(article)\n",
    "                    data.append({\n",
    "                        'date': post_date,\n",
    "                        'reaction_count': reaction_count,\n",
    "                        'top_react': top_react,\n",
    "                        'comment_count': comment_count,\n",
    "                        'post_link': post_link\n",
    "                    })\n",
    "\n",
    "            self.click_see_more_link()\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def extract_full_post_content(self, post_link):\n",
    "        try:\n",
    "            self.driver.get(post_link)\n",
    "            time.sleep(random.uniform(2, 5))\n",
    "\n",
    "            paragraphs = self.driver.find_elements(By.TAG_NAME, 'p')\n",
    "            full_content = ' '.join([p.text.strip() for p in paragraphs])\n",
    "\n",
    "            hashtags = [word for word in full_content.split() if word.startswith('#')]\n",
    "            emojis = ''.join(c for c in full_content if not c.isalnum() and c not in [' ', '#'])\n",
    "\n",
    "            return full_content, emojis, hashtags\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting full post content from {post_link}: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    def scrape_full_content_for_all_posts(self, df):\n",
    "        full_content_data = []\n",
    "        for index, row in df.iterrows():\n",
    "            post_link = row['post_link']\n",
    "            if post_link:\n",
    "                full_content, emojis, hashtags = self.extract_full_post_content(post_link)\n",
    "                full_content_data.append({\n",
    "                    'post_link': post_link,\n",
    "                    'full_content': full_content,\n",
    "                    'emojis': emojis,\n",
    "                    'hashtags': hashtags\n",
    "                })\n",
    "                time.sleep(random.uniform(3, 7))\n",
    "            else:\n",
    "                full_content_data.append({\n",
    "                    'post_link': post_link,\n",
    "                    'full_content': None,\n",
    "                    'emojis': None,\n",
    "                    'hashtags': None\n",
    "                })\n",
    "        return pd.DataFrame(full_content_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FacebookScraper` class is designed to automate the process of scraping Facebook posts using Selenium WebDriver. It starts by initializing the WebDriver with Chrome options such as disabling notifications and running in headless mode, allowing for smooth and invisible scraping without pop-ups. The class stores user credentials for logging into Facebook and includes a `login` method that navigates to Facebook's login page, enters the user's credentials, and waits for the login process to complete.\n",
    "\n",
    "Once logged in, the class uses various methods to extract information from Facebook posts. These include `parse_post_date` to retrieve the date of a post, `extract_reaction_type` to identify the types of reactions (e.g., \"Thích\", \"Yêu thích\"), and `extract_reaction_count` to calculate the number of reactions by traversing the DOM tree. Similarly, the `extract_comment_count` method extracts the number of comments, while `extract_post_link` retrieves the post's URL. The class also includes pagination handling through the `click_see_more_link` method, which clicks on the \"Xem tin khác\" (\"See more posts\") link to load additional content, with delays to ensure smooth operation.\n",
    "\n",
    "The core method, `scrape_facebook_page`, orchestrates the entire scraping process by iterating through multiple pages of posts, extracting relevant details like reaction counts and post links, and compiling them into a DataFrame. The class also provides a `close` method to properly terminate the WebDriver session when scraping is complete. Additionally, for extracting full post content, including emojis and hashtags, the `extract_full_post_content` method navigates to each post's URL, retrieves paragraph text, identifies emojis and hashtags, and returns these along with the full content. \n",
    "\n",
    "Lastly, the `scrape_full_content_for_all_posts` method processes a DataFrame of post links, extracting detailed content for each post and handling delays between requests to avoid being blocked by Facebook. The class provides a comprehensive solution for scraping large amounts of Facebook post data efficiently, handling pagination, extracting detailed content, and ensuring a reliable scraping process with proper resource management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in terms of extracting emojis, the authors aim at extracting features which are not words and hashtags sign (#), thus the emojis feature contains punctuation marks and other non-emojis. In terms of posting date, posts that are posted in 2024, the year 2024 is not included in the post” posting date. \n",
    "\n",
    "Thus, the authors create functions for handling two specific challenges: cleaning emoji data and parsing inconsistent date formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean emojis by removing unwanted characters\n",
    "def clean_emojis(emojis):\n",
    "    if emojis:\n",
    "        emoji_pattern = r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\u2728]'\n",
    "        cleaned_emojis = ''.join(re.findall(emoji_pattern, emojis))\n",
    "        return cleaned_emojis if cleaned_emojis else None\n",
    "    return None\n",
    "# Function to parse dates and handle missing years\n",
    "def parse_date(date_str):\n",
    "    if isinstance(date_str, pd.Timestamp):\n",
    "        return date_str  # If it's already a timestamp, return as is\n",
    "    if pd.isna(date_str) or not isinstance(date_str, str) or not date_str.strip():\n",
    "        return pd.NaT\n",
    "\n",
    "    date_str = date_str.strip()\n",
    "    current_year = datetime.now().year\n",
    "    \n",
    "    # Attempt to parse with the assumption of missing year\n",
    "    try:\n",
    "        return datetime.strptime(f\"{date_str} {current_year}\", \"%d tháng %m lúc %H:%M %Y\")\n",
    "    except ValueError:\n",
    "        # Handle cases where the year is included\n",
    "        try:\n",
    "            return datetime.strptime(date_str, \"%d tháng %m, %Y lúc %H:%M\")\n",
    "        except ValueError:\n",
    "            return pd.NaT\n",
    "\n",
    "# Function to format the post data\n",
    "def format_post_data(df):\n",
    "    # Clean emojis in the 'emojis' column\n",
    "    df['emojis'] = df['emojis'].apply(clean_emojis)\n",
    "    \n",
    "    # Parse the 'date' column\n",
    "    df['date'] = df['date'].apply(parse_date)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    username = \"your-facebook-email\"\n",
    "    password = \"your-password\"\n",
    "\n",
    "    scraper = FacebookScraper(username, password)\n",
    "    scraper.login()\n",
    "    scraper.driver.get('https://mbasic.facebook.com/your-page?v=timeline')\n",
    "\n",
    "    # Scrape initial post data\n",
    "    your-page = scraper.scrape_facebook_page()\n",
    "\n",
    "    # Scrape full content for each post\n",
    "    full_content_df = scraper.scrape_full_content_for_all_posts(your-page)\n",
    "    your-page = pd.merge(your-page, full_content_df, on='post_link', how='left')\n",
    "    your-page.drop(columns='post_link', inplace=True)\n",
    "    scraper.close()\n",
    "\n",
    "    your-page_crawl_data = format_post_data(your-page)\n",
    "your-page_crawl_data.to_csv('your-page_crawl_data.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function automates the process of logging into Facebook and scraping posts from the Heineken Vietnam Facebook page using a `FacebookScraper` class. It begins by checking if the script is being run as the main program (`if __name__ == \"__main__\":`). If so, it starts by assigning the username and password for a Facebook account to the variables `username` and `password`.\n",
    "\n",
    "A `FacebookScraper` instance is then created using the provided credentials, and the `login()` method is called to log into Facebook. After logging in, the WebDriver is directed to the mobile version of the Heineken Vietnam Facebook page (`https://mbasic.facebook.com/your-page?v=timeline`), which is a lightweight version of Facebook suitable for scraping.\n",
    "\n",
    "The next step involves scraping initial post data from the page using the `scrape_facebook_page()` method, which gathers details like post date, reaction count, comment count, and top reactions. The resulting data is stored in a DataFrame named `your-page`.\n",
    "\n",
    "After collecting the basic post data, the script proceeds to scrape the full content of each post (including emojis and hashtags) using the `scrape_full_content_for_all_posts()` method. The scraped full content is stored in another DataFrame (`full_content_df`). This DataFrame is then merged with the initial `your-page` DataFrame based on the post links (`post_link`), using a left join to combine both datasets. Once merged, the `post_link` column is dropped as it is no longer needed.\n",
    "\n",
    "The script then closes the WebDriver session with the `scraper.close()` method to free up resources. Finally, the `format_post_data(your-page)` function is called to format the scraped data, and the final DataFrame (`your-page_crawl_data`) is saved to a CSV file named `your_page_crawl_data.csv`. The CSV file contains all the scraped data, including post details and full content, ready for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heineken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    username = \"your-facebook-email\"\n",
    "    password = \"your-password\"\n",
    "\n",
    "    scraper = FacebookScraper(username, password)\n",
    "    scraper.login()\n",
    "    scraper.driver.get('https://mbasic.facebook.com/HEINEKENVietnam?v=timeline')\n",
    "\n",
    "    # Scrape initial post data\n",
    "    heineken = scraper.scrape_facebook_page()\n",
    "\n",
    "    # Scrape full content for each post\n",
    "    full_content_df = scraper.scrape_full_content_for_all_posts(heineken)\n",
    "    heineken = pd.merge(heineken, full_content_df, on='post_link', how='left')\n",
    "    heineken.drop(columns='post_link', inplace=True)\n",
    "    scraper.close()\n",
    "\n",
    "    heineken_crawl_data = format_post_data(heineken)\n",
    "heineken_crawl_data.to_csv('heineken_crawl_data.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 333 Beer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    username = \"your-facebook-email\"\n",
    "    password = \"your-password\"\n",
    "\n",
    "    scraper = FacebookScraper(username, password)\n",
    "    scraper.login()\n",
    "    scraper.driver.get('https://mbasic.facebook.com/bia333.sabeco?v=timeline')\n",
    "\n",
    "    # Scrape initial post data\n",
    "    beer333 = scraper.scrape_facebook_page()\n",
    "\n",
    "    # Scrape full content for each post\n",
    "    full_content_df = scraper.scrape_full_content_for_all_posts(beer333)\n",
    "    beer333 = pd.merge(beer333, full_content_df, on='post_link', how='left')\n",
    "    beer333.drop(columns='post_link', inplace=True)\n",
    "    scraper.close()\n",
    "\n",
    "    beer333_crawl_data = format_post_data(beer333)\n",
    "beer333_crawl_data.to_csv('beer333_crawl_data.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bia Saigon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    username = \"your-facebook-email\"\n",
    "    password = \"your-password\"\n",
    "\n",
    "    scraper = FacebookScraper(username, password)\n",
    "    scraper.login()\n",
    "    scraper.driver.get('')\n",
    "\n",
    "    # Scrape initial post data\n",
    "    biasg = scraper.scrape_facebook_page('https://mbasic.facebook.com/BiaSaigon.official.page?v=timeline')\n",
    "\n",
    "    # Scrape full content for each post\n",
    "    full_content_df = scraper.scrape_full_content_for_all_posts(biasg)\n",
    "    biasg = pd.merge(biasg, full_content_df, on='post_link', how='left')\n",
    "    biasg.drop(columns='post_link', inplace=True)\n",
    "    scraper.close()\n",
    "\n",
    "    biasg_crawl_data = format_post_data(biasg)\n",
    "biasg_crawl_data.to_csv('biasg_crawl_data.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill the required information*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
